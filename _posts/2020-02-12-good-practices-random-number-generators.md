---
layout: single
author_profile: true
title: Using numpy random number generators
excerpt_separator: <!--more-->
---

I wanted to write this blog post because I found it hard to find a good and simple resource about how to properly use a random number generator in python. I was able to find some great resources online but never what I was looking for and that I therefore put here. Also, we are often told to seed a random number generator for the sake of reproducibility. Indeed using the same seed should ensure that each time you run your code your obtain exactly the same results. Correctly seeding a random number generator can sometimes be harder than you think, for instance when running your code in parallel. 

A lot of computation in machine learning rely on randomness, including data generation, data preprocessing, cross-validation, optimization algorithms such as stochastic gradient descent, random initialization (for instance for neural networks). <!--more--> One also wants to know whether his results hold independently of this randomness. Specifically, the results will most likely be true on average or with great probability. To assess the performance of his algorithm or idea, one usually repeats the experiment several times. Relying on randomness means using Pseudo Random Number Generators (PRNGs), unless you are working on a problem where you can afford a true random number generator. In this post I will present a few things that are useful to be aware of for a good use of a PRNG and especially when using the RNG available in numpy. I will assume that numpy 1.17 or greater is used. The reason for this is that great new features were introduced in the [numpy.random module]([numpy.random](https://numpy.org/doc/1.18/reference/random/index.html) module) in version 1.17 (more on these new features below). I also assume that `numpy` is imported as `np` and will use `np` in the rest of the blog post.

### TL, DR.
1. Do not use the implicit global random number generator and the `np.random` functions such as `np.random.seed`
2. Create a new random number generator and pass it around.

Finally, we want to be able to reproduce our results, for the sake of science but also more simply for the sake of our jobs of debuggers.

## Random number generation with numpy
Generating random numbers in numpy is handled in the [np.random](https://numpy.org/doc/1.18/reference/random/index.html) module which introduced great new features since numpy 1.17 (more about this later). When you import numpy in your python script, a new random generator that I will here called the global random generator is created. This global random generator is created with a global random state. Each time you generate a new random value using `np.random`, this global random number generator and its global random state are used. You can also control the state of this global random generator using the function `np.random.seed`. You can see scripts where people set the seed at the beginning using this function and then use `np.random` to generate the random values that they want. Fixing the seed at the beginning ensures that the script is reproducible: the same values and results should be produced each time you run it. However using the global random generator and the global random state is considered a bad practice, for the simple reason that using global variables can lead to undesired side effects. For instance one might use `np.random` without knowing that the state was set somewhere else in its codebase. Using the global random state can however sometimes be convenient. Looking at the [NEP](https://numpy.org/neps/nep-0019-rng-policy.html#numpy-random) related to the new features of the `np.random` module you can see that it is explicitly written that that the implicit global random state and the `np.random` functions should not be avoided.

> The implicit global RandomState behind the `np.random.*` convenience functions can cause problems, especially when threads or other forms of concurrency are involved. Global state is always problematic. We categorically recommend avoiding using the convenience functions when reproducibility is involved.

The recommended practice is written in this NEP

> The preferred best practice for getting reproducible pseudorandom numbers is to instantiate a generator object with a seed and pass it around.

To summarize:
* Instead of using `np.random.seed`, which reseeds the already created implicit numpy global random number generator and then using `np.random` functions you should create a new random number generator.
* You should create one random number generator with a seed at the beginning of your script and use this RNG in the rest of your script.

The reason for this practice is that you can loose on the randomness and the independence of the generated random numbers by reseeding the RNG multiple times. Furthermore if you need a good seed, this can sometimes take time to obtain one. Once you have a good seed to instantiate your generator, you might as well use it. With a good RNG such as the one of numpy you will be ensured good randomness (and independence) of the generated numbers. It might be more dangerous to use different seeds. How do you know that the streams of random numbers obtained with two different seeds are not correlated, or I should say less independent than the ones created from the same seed.

As you write functions that you will use on their own as well as in a more complex script you may want to be able to specify a seed or pass your already created random generator. A function that I find very handy for this is the scikit-learn function [`sklearn.utils.check_random_state`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html). While writing this post I discovered that this function is now available in [scipy](https://github.com/scipy/scipy/blob/master/scipy/_lib/_util.py#L173). A look at the docstring and/or the source code of this function will give you a good sense about what it is doing. This functions turns an input seed argument into a random number generator. If the seed is `None`, then the function returns the already existing (**is it really true that it already exists?**) numpy global random number generator. This can be convenient because if you fixed the seed before in your script using `np.random.seed`, the function returns the generator that was seeded at the beginning of the script. This is one of the thing that I find convenient with using the global random number generator. Although as explained above this is not the recommended practice and you can use this at your own risk. Coming back to the `check_random_state` function, if the seed is an int, this will create a new random state. This is very convenient if you want to test or use the function on its own and do not have an already created random number generator. Finally if you pass an already created random number generator as the seed argument then the function will return it. I basically use the `check_random_state` function in all my own functions that depend on a random number generator.

From numpy 1.17 you can now use the `default_rng()` function. However the difference is that if `None` is passed then a new random number generator is instantiated instead of returning the global random number generator. For best practices this is a good thing, even though as written above it can be convenient to be able to use the global random number generator.

## Parallel processing

One important thing to talk about is parallel processing. When running my code in parallel I usually use the [joblib](https://joblib.readthedocs.io/en/latest/index.html) library so I will mainly talk about it but most of the discussion also applies more generally. You must be careful when using random number generators in conjunction with parallel processing. Depending on the parallel processing library or backend that you use different behaviors can be observed. For instance if you rely on the global numpy random state to generate random numbers in parallel, it can be the case that forked Python processes use the same random seed and thus produce the exact same results which is clearly not what you want and a waste of computational resources. A very nice example showing the different behaviors that one can obtain with joblib is available [here](https://joblib.readthedocs.io/en/latest/auto_examples/parallel_random_state.html). As written in the introduction of this post, it is a good practice to fix the seed for reproducibility. If you fix the seed at the beginning of your main scrip and then pass the same RNG to each process to be run in parallel, most of the time this will not give you what you want as this RNG will be deep copied and thus the same results will be produced by each process (**check this**). One of the solutions is to create as many RNGs as parallel processes and choose one different seed for each of these RNGs. The issue now is that you cannot choose the seeds as easily as you can originally think. When you choose two different seeds to instantiate two different RNGs how do you know that the numbers produced by these RNGs will appear as statistically independent. The design of independent RNGs for parallel processes has been an important research question. You can for instance refer to the paper, [Random numbers for parallel computers: Requirements and methods, with emphasis on GPUs](https://www.sciencedirect.com/science/article/pii/S0378475416300829) by L'Ecuyer et al. (2017) for a good summary of the different methods on this topic.

From numpy 1.17, this has become very easy to instantiate independent random number generators. Depending of the RNG that you use, different strategies are easily available as documented in the [Parallel generation section](https://docs.scipy.org/doc/numpy/reference/random/index.html?highlight=numpy%20random#parallel-generation) of the numpy documentation. One of the strategies is to use `SeedSequence` which is an algorithm that makes sure that a not so good user-provided seed results in a good initial state for the RNG. Additionally, it ensures that two close seeds will result in two very different initial states for the RNG that are independent of each other. You can refer to the documentation of [SeedSequence Spawning](https://docs.scipy.org/doc/numpy/reference/random/parallel.html#seedsequence-spawning) for an example on how to generate independent RNGs from a user-provided seed. I here provide an example illustrating how to use this with the joblib example mentioned above.

**add small example with joblib and parallel seeds, use the example given in joblib**


### Resources

#### About numpy RNGs
* [The documentation of the numpy random module](https://docs.scipy.org/doc/numpy/reference/random/index.html?highlight=numpy%20random) is the best place to find information and where I found most of the information that I shared here.
* [The NEP about the Random Number Generator Policy of Numpy](https://numpy.org/neps/nep-0019-rng-policy.html) which lead to the changes introduced in numpy 1.17
* A [recent numpy issue](https://github.com/numpy/numpy/issues/15322) about the `check_random_state` and random number generator good practices.

#### About RNGs more generally
* [Random numbers for parallel computers: Requirements and methods, with emphasis on GPUs](https://www.sciencedirect.com/science/article/pii/S0378475416300829) by L'Ecuyer et al. (2017)
* To know more about the default RNG used in numpy, named PCG, I recommend the [PCG paper](https://www.pcg-random.org/paper.html) which also contain lots of useful information about RNG in general. The [pcg-random.org website](https://www.pcg-random.org) is also full of interesting information about RNGs.